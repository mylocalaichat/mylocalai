# Local AI Chat with LangGraph & MCP Tools

A Next.js chat application powered by **LangGraph** with **MCP (Model Context Protocol)** tools for real-time web search and data access. Features **Server-Sent Events (SSE)** streaming for real-time AI responses. Completely local and open source.

[![GitHub](https://img.shields.io/badge/GitHub-Source%20Code-181717?style=flat&logo=github)](https://github.com/mylocalaichat/mylocalai)

**[ðŸŽ¥ Watch Demo](https://youtu.be/g14zgT6INoA)**

## Architecture

### Core Components
- **Next.js 15** - Modern React framework with App Router
- **LangGraph** - AI agent framework for complex reasoning workflows
- **MCP Tools** - Google search, web scraping, and data access tools
- **SSE Streaming** - Real-time response streaming via Server-Sent Events
- **Ollama** - Local LLM hosting (Qwen 3 14B recommended)

### Data Flow
```
User Message â†’ LangGraph Agent â†’ MCP Tools (Web Search) â†’ LLM â†’ SSE Stream â†’ UI
```

## Features

### ðŸš€ Advanced AI Capabilities
- âœ… **LangGraph Agent** - Complex reasoning and tool usage
- âœ… **Real-time Web Search** - Current information via Google Search
- âœ… **Web Scraping** - Extract content from specific URLs
- âœ… **SSE Streaming** - Real-time response updates
- âœ… **Tool Call Visibility** - See when AI uses external tools

### ðŸ’¬ Chat Interface
- âœ… **Markdown Rendering** - Rich text responses with code highlighting
- âœ… **Conversation History** - Persistent chat threads
- âœ… **Multiple Sessions** - Manage multiple conversations
- âœ… **Real-time Status** - Live updates during processing

### ðŸ”’ Privacy & Performance
- âœ… **Completely Local** - AI runs on your hardware
- âœ… **No API Keys Required** - Uses local Ollama instance
- âœ… **Thread Management** - SQLite-based conversation storage
- âœ… **Debug Mode** - Detailed logging and performance metrics

## Quick Start

### 1. Install Ollama
```bash
# macOS
brew install ollama

# Windows/Linux
# Visit https://ollama.com for installer
```

### 2. Install Required Model
```bash
ollama pull qwen3:14b
# Alternative: ollama pull llama3.1:8b
```

### 3. Start Ollama Service
```bash
ollama serve
```

### 4. Run the Application
```bash
npm install
npm run dev
```

### 5. Open Browser
Navigate to [http://localhost:3000](http://localhost:3000)

## Technical Requirements

### Hardware
- **RAM**: 16GB+ (recommended for Qwen 3 14B)
- **CPU**: Modern multi-core processor
- **Storage**: 10GB+ free space for models

### Software
- **Node.js**: v18+ (v20+ recommended)
- **Ollama**: Latest version
- **Modern Browser**: Chrome, Firefox, Safari, Edge

### Key Dependencies
- **Next.js 15** - React framework with App Router
- **LangGraph** - AI agent framework (@langchain/langgraph ^0.4.9)
- **MCP SDK** - Model Context Protocol (@modelcontextprotocol/sdk ^1.18.1)
- **Ollama LangChain** - Local LLM integration (@langchain/ollama ^0.2.4)
- **SQLite Checkpointer** - Conversation persistence (@langchain/langgraph-checkpoint-sqlite ^0.2.1)

## Configuration

### Environment Variables
Create `.env.local`:
```bash
REACT_APP_OLLAMA_URL=http://localhost:11434
PORT=3000
NODE_ENV=development
```

### Model Configuration
Edit `app/page.tsx` to change the model:
```typescript
const requiredModel = 'qwen3:14b';
// or 'llama3.1:8b', 'llama3.1:70b', etc.
```

## MCP Tools Available

### Google Search (`google_search`)
- **Purpose**: Get current information from web search
- **Usage**: Automatically used for current events, facts, news
- **Parameters**: `query` (string)

### Web Scraper (`scrape`)
- **Purpose**: Extract content from specific URLs
- **Usage**: Get detailed information from websites
- **Parameters**: `url` (string)

### Dice Roller (`roll_dice`)
- **Purpose**: Generate random numbers
- **Usage**: Games, randomization, decision making
- **Parameters**: `sides` (number), `count` (number)

## Development

### Available Scripts
```bash
npm run dev          # Development server with hot reload
npm run build        # Production build
npm run start        # Production server
npm run lint         # ESLint code checking
npm test             # Run test suite
```

### Project Structure
```
app/
â”œâ”€â”€ components/           # React components
â”‚   â”œâ”€â”€ ChatInterface.tsx # Main chat UI
â”‚   â”œâ”€â”€ ChatList.tsx     # Conversation sidebar
â”‚   â””â”€â”€ StatusBanner.tsx # Connection status indicator
â”œâ”€â”€ langraph_backend/    # LangGraph API routes
â”‚   â”œâ”€â”€ route.ts        # Main SSE streaming endpoint
â”‚   â”œâ”€â”€ schemas.ts      # Request/response validation
â”‚   â”œâ”€â”€ lib/            # Utilities and checkpointer
â”‚   â””â”€â”€ conversations/  # Thread management API
â”‚       â”œâ”€â”€ route.ts    # List conversations
â”‚       â””â”€â”€ [thread_id]/route.ts # Get/delete specific conversation
â”œâ”€â”€ mcp_server/         # MCP tool implementations
â”‚   â”œâ”€â”€ [transport]/    # MCP protocol handler
â”‚   â”‚   â””â”€â”€ route.ts    # Tool registration and routing
â”‚   â”œâ”€â”€ tools/         # Individual tool definitions
â”‚   â”‚   â”œâ”€â”€ googleSearch.ts # Google search tool
â”‚   â”‚   â”œâ”€â”€ scrape.ts   # Web scraping tool
â”‚   â”‚   â””â”€â”€ rollDice.ts # Random number generator
â”‚   â”œâ”€â”€ search/        # Google search implementation
â”‚   â””â”€â”€ scrape/        # Web scraping implementation
â”œâ”€â”€ utils/             # Shared utilities
â”‚   â””â”€â”€ localStorage.ts # Browser storage helpers
â”œâ”€â”€ layout.tsx         # Root layout component
â””â”€â”€ page.tsx           # Main chat page
```

### Adding New MCP Tools
1. Create tool definition in `app/mcp_server/tools/`
2. Register in `app/mcp_server/[transport]/route.ts`
3. Tool will be automatically available to LangGraph agent

## Troubleshooting

### Ollama Issues
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# List installed models
ollama list

# Check model installation
ollama pull qwen3:14b
```

### Performance Optimization
- **Reduce Model Size**: Use `qwen3:7b` or `llama3.1:8b` for lower memory usage
- **Close Applications**: Free up RAM for better model performance
- **Check Resources**: Monitor CPU/RAM usage during chat

### SSE Streaming Issues
- Check browser console for connection errors
- Verify LangGraph backend is running on port 3000
- Ensure no firewall blocking Server-Sent Events

### MCP Tool Errors
- Verify MCP server is connected in logs
- Check tool implementation in `app/mcp_server/tools/`
- Ensure Google search API is accessible

## API Endpoints

### Chat Streaming
- **POST** `/langraph_backend` - SSE streaming chat endpoint
- **Headers**: `Content-Type: application/json`, `Accept: text/event-stream`

### Conversation Management
- **GET** `/langraph_backend/conversations` - List all conversations
- **GET** `/langraph_backend/conversations/[id]` - Get specific conversation
- **DELETE** `/langraph_backend/conversations/[id]` - Delete conversation

### MCP Tools
- **POST** `/mcp_server/mcp` - MCP protocol endpoint for tools

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

MIT License - See [LICENSE](LICENSE) file for details.

## Acknowledgments

- [Ollama](https://ollama.com) - Local LLM hosting
- [LangGraph](https://langchain-ai.github.io/langgraph/) - AI agent framework
- [Model Context Protocol](https://modelcontextprotocol.io/) - Tool integration standard
- [Next.js](https://nextjs.org/) - React framework